# 강화학습 (Reinforcement Learning) - p50
어떤 환경에서 어떠한 행동을 했을 때 결과를 판단하고 보상(또는 벌칙)을 주는 행동을 반복해서 스스로 학습


## 마르코프 의사 결정과정 (Markov Decision Process, MDP)


### 마르코프 가정 (Markov Assumption) - p50
현재의 상태는 바로 이전 상태에만 영향을 받는다


### 마르코프 과정 (Markov Process)
- 마르코프 가정을 만족하는 연속적인 일련의 상태
- 상태 전이 확률 (State Transition Probability) : 어떠한 상태 i가 다음 상태 j가 될 확률


### 마르코프 의사 결정과정
- 마르코프 과정을 기반으로한 의사 결정 모델

```
MDP = (S, A, P, R, γ)
```

- S : 상태(state) 집합
- A : 행동(action) 집합
- P : 상태 전이 확률(state transition Probability) 행렬
- R : 보상(Reward) 함수
- γ : 할인 요인(discount factor)


## 상태 가치 함수 & 상태-행동 가치 함수 - p54


### 상태 가치 함수 (state-value function)
- 현재 상태 s에서 정책 π를 따랐을 때의 가치를 반환


### 상태-행동 가치 함수 (action-value function)
- 어떤 상태 s에서 행동 a를 수행했을 때의 가치를 반환


## 벨만 방정식 (Bellman Equation) - p55
'상태 가치 함수'와 '상태-행동 가치 함수'의 관계를 나타내는 방정식


### 벨만 기대 방정식 (Bellman Expection Equation)
- '상태 가치 함수'의 벨만 기대 방정식
  - 현재 상태의 가치는 다음 상태의 가치에 할인률을 곱해 더한 기댓값
- '상태-행동 가치 함수'의 벨만 기대 방정식
  - 현재 상태와 행동의 '상태-행동 가치'는 다음 상태와 행동의 '상태-행동 가치'에 할인률을 곱한 값을 기대 보상에 더한 것

### 벨만 최적 방정식 (Bellman Optimality Equation)
최적 가치라는 의미는 가장 큰 총 보상을 받을 수 있는 정책을 따랐을 때 얻을 가치를 뜻합니다.


## MDP를 위한 동적 프로그래밍 (Dynamic Programming) - p57
벨만 방정식을 이용해 MDP를 푸는 동적 프로그래밍


### 정책 반복 (Policy Iteration)
- 상태 가치 함수를 반복


### 가치 반복 (Value Iteration)
정책 반복과 유사

- 정책 반복 : 다음 상태의 가치를 정책 함수의 확률과 곱해 모두 더함
- 가치 반복 : 탐욕적으로 가장 큰 다음 가치를 선택 (벨만 최적 방정식)


### 동적 프로그래밍의 한계와 강화 학습
- 동적 프로그래밍 = MDP의 상태 전이 확률과 보상 함수가 주어져야 하기 때문에 미리 알 수 없는 대부분의 현실 문제에 적용 어려움
- 상태 전이 확률과 보상 함수를 미리 알더라도, 상태의 수가 많아지면 어려움 (특히, 빅데이터라면...)
- 그래서 강화 학습으로 접근하면 극복 가능


## 강화학습 (Reinforcement Learning) - p61


### 강화학습 표기법 (notation)
- `s` : 강화학습 환경에서의 상태
- `a` : 에이전트가 수행할 수 있는 행동
- `P` : 상태 전이 확률
- `V` : 상태 가치
- `π` : 정책
- `Q` : 상태-행동 가치 함수
- `R` : 보상 함수


### Model-based vs. Model-free
MDP에서 상태 전이 확률과 보상 함수 = `모델(model)`

- Model-based : 상태 전이 확률과 보상 함수를 정할 수 있는 경우. MDP와 동적 프로그래밍이 해당
- Model-free : 모델 없이 하는 강화 학습. 대부분의 강화학습


### 예측(Prediction)과 제어(Control)
강화학습에서는 모델을 미리 알 수 없기 때문에, `상태 가치 함수`의 값을 예측




