# 강화학습 (Reinforcement Learning) - p50
어떤 환경에서 어떠한 행동을 했을 때 결과를 판단하고 보상(또는 벌칙)을 주는 행동을 반복해서 스스로 학습


## 마르코프 의사 결정과정 (Markov Decision Process, MDP)


### 마르코프 가정 (Markov Assumption) - p50
현재의 상태는 바로 이전 상태에만 영향을 받는다


### 마르코프 과정 (Markov Process)
- 마르코프 가정을 만족하는 연속적인 일련의 상태
- 상태 전이 확률 (State Transition Probability) : 어떠한 상태 i가 다음 상태 j가 될 확률


### 마르코프 의사 결정과정
- 마르코프 과정을 기반으로한 의사 결정 모델

```
MDP = (S, A, P, R, γ)
```

- S : 상태(state) 집합
- A : 행동(action) 집합
- P : 상태 전이 확률(state transition Probability) 행렬
- R : 보상(Reward) 함수
- γ : 할인 요인(discount factor)


## 상태 가치 함수 & 상태-행동 가치 함수 - p54


### 상태 가치 함수 (state-value function)
- 현재 상태 s에서 정책 π를 따랐을 때의 가치를 반환


### 상태-행동 가치 함수 (action-value function)
- 어떤 상태 s에서 행동 a를 수행했을 때의 가치를 반환


## 벨만 방정식 (Bellman Equation) - p55
'상태 가치 함수'와 '상태-행동 가치 함수'의 관계를 나타내는 방정식


### 벨만 기대 방정식 (Bellman Expection Equation)
- '상태 가치 함수'의 벨만 기대 방정식
  - 현재 상태의 가치는 다음 상태의 가치에 할인률을 곱해 더한 기댓값
- '상태-행동 가치 함수'의 벨만 기대 방정식
  - 현재 상태와 행동의 '상태-행동 가치'는 다음 상태와 행동의 '상태-행동 가치'에 할인률을 곱한 값을 기대 보상에 더한 것

### 벨만 최적 방정식 (Bellman Optimality Equation)
최적 가치라는 의미는 가장 큰 총 보상을 받을 수 있는 정책을 따랐을 때 얻을 가치를 뜻합니다.


## MDP를 위한 동적 프로그래밍 (Dynamic Programming) - p57
벨만 방정식을 이용해 MDP를 푸는 동적 프로그래밍


### 정책 반복 (Policy Iteration)
- 상태 가치 함수를 반복


### 가치 반복 (Value Iteration)
정책 반복과 유사

- 정책 반복 : 다음 상태의 가치를 정책 함수의 확률과 곱해 모두 더함
- 가치 반복 : 탐욕적으로 가장 큰 다음 가치를 선택 (벨만 최적 방정식)


### 동적 프로그래밍의 한계와 강화 학습
- 동적 프로그래밍 = MDP의 상태 전이 확률과 보상 함수가 주어져야 하기 때문에 미리 알 수 없는 대부분의 현실 문제에 적용 어려움
- 상태 전이 확률과 보상 함수를 미리 알더라도, 상태의 수가 많아지면 어려움 (특히, 빅데이터라면...)
- 그래서 강화 학습으로 접근하면 극복 가능


## 강화학습 (Reinforcement Learning) - p61


### 강화학습 표기법 (notation)
- `s` : 강화학습 환경에서의 상태
- `a` : 에이전트가 수행할 수 있는 행동
- `P` : 상태 전이 확률
- `V` : 상태 가치
- `π` : 정책
- `Q` : 상태-행동 가치 함수
- `R` : 보상 함수


### Model-based vs. Model-free
MDP에서 상태 전이 확률과 보상 함수 = `모델(model)`

- Model-based : 상태 전이 확률과 보상 함수를 정할 수 있는 경우. MDP와 동적 프로그래밍이 해당
- Model-free : 모델 없이 하는 강화 학습. 대부분의 강화학습


### 예측(Prediction)과 제어(Control)
강화학습에서는 모델을 미리 알 수 없기 때문에, '상태 가치 함수'의 값을 `예측` → 예측이 정확해지도록 `제어`

- `예측` : 주어진 정책에 따라 환경과의 소통을 통해 '상태 가치 함수'를 학습
  - 모든 상태에 대한 가치 판단 어려움 → 샘플링(sampling)
- `제어` : 예측을 통해 학습한 가치 함수를 기반으로 정책을 학습


### 부트스트랩 (Bootstrap)
다음 상태에 대한 가치 함숫값으로 현재 상태의 가치 함숫값을 예측하는 방식

- `에피소드(Episode)` : 강화학습에서 '종료 상태(Terminal State)'까지의 상태 전이가 진행된 것
  - 부트스트랩을 사용하면 에피소드가 진행되는 동안에 상태 가치 함수 업데이트 가능


### 'On-policy' vs. 'Off-policy'
- `On-policy` : 행동을 결정하는 정책과 학습할 정책이 같은 강화학습
  - 경험을 쌓아도 정책이 업데이트 되지 않으면 이 경험을 학습에 사용할 수 없음
- `Off-policy` : 행동하는 정책과 학습하는 정책이 다른 방법
  - 데이터 효율 관점에서는 우위에 있음


### '이용(Exploitation)'과 '탐험(Exploration)'
- `이용(Exploitation)` : 현재 알고 있는 한 가장 최적의 행동을 선택
  - 최적의 행동은 아닐 수 있음 → 탐욕(Greedy)적으로 행동을 선택
- `탐험(Exploration)` : 다양한 경험을 쌓기 위해 아무 행동이나 선택
  - 탐욕적으로 행동을 선택하다 보면 최적인 행동을 전혀 선택하지 못할 수도 있음 → 이런 문제를 해결하기 위해 간혹 탐험


## 주요 강화학습 기법
이 책에서는 가치와 정책을 신경망을 통해 구하는 강화학습 기법만 사용


### 몬테카를로 학습 (Monte-Carlo learning, MC)
에피소드가 끝나야지 총 획득 보상 G를 알 수 있기 때문에 에피소드 종료 시점에서 상태 가치 함수 갱신 가능


### 시간차 학습 (Temporal-Difference Learning, TD)
타임 스텝(time step)마다 함수 업데이트 가능

- `SARSA(State Action Reward State Action)` : On-policy 시간차 제어 (Temporal-Difference Control)
  - TD(시간차 학습)에서 상태-행동 가치 함수를 적용하고 탐험(Exploration)을 추가해 제어
  - 하나의 샘플이 현재 state, action, reward, 다음 state, 다음 action으로 구성
  - SARSA는 다음 상태에 대한 행동을 알고 있어야 하기 때문에 행동을 결정하는 정책과 학습 정책을 분리할 수 없으므로 On-policy 방식이라고 할 수 있음
- `Q-Learning` : Off-policy 시간차 제어 (Temporal-Difference Control)


### 'Q-러닝(Q-Learning, QL)'과 'DQN(deep Q-Network)'
Off-policy 시간차 제어 (Temporal-Difference Control)

- 행동을 결정할 때는 Q값이 높은 행동을 선택하면서도 충분한 탐험을 수행
- 탐험은 학습 초기에는 많이 하고 학습 후반으로 갈수록 적게 하는 기법을 많이 사용

- Q-Learning에서 Q함수로 심층 신경망을 사용하면 DQN
  - DQN은 회귀(Regression) 문제를 푸는 대표적인 강화학습


### 정책 경사 (Policy Gradient, PG)
분류(Classification) 문제를 푸는 강화학습 → 어떠한 상태에서 어떠한 행동을 결정하는 것이 가장 좋을지 판단

- Q-Learning에서는 Q함수를 학습하고 Q값이 높은 행동을 취하는 방식
- 정책 경사에서는 행동의 확률을 학습


### 액터-크리틱 (Actor-Critic)
정책 경사 방식의 변형. Q-Learning과 정책 경사의 하이브리드 강화학습 모델.

- `액터(Actor)` : 정책 경사 모델을 사용해 수행할 행동을 결정
- `크리틱(Critic)` : 수행한 행동을 비평(criticism)하기 위해 Q-Learning 모델을 사용

- 단일 모델을 사용했을 때 발생하기 쉬운 고분산(High Variance) 문제 회피 가능


### A2C (Advantage Actor-Critic)
액터-크리틱 방법에서 기대 출력을 어드밴티지로 사용하는 방법

- `어드밴티지(Adavantage)` : 상태-행동 가치에서 상태 가치를 뺀 값
- 크리틱을 어드밴티지로 학습 → 어떠한 상태에서 행동이 얼마나 좋은지 뿐만 아니라 얼마나 더 좋아지는지를 학습할 수 있음


### A3C (Asynchronous Advantage Actor-Critic)
구글 딥마인드에서 발표한 대표적인 강화학습 방법 중 하나. A2C에서 비동기 개념을 추가

- 에이전트가 여러 개의 환경에서 동시에 행동을 수행하고 신경망을 학습 = 여러 환경에서 동시에 신경망 학습 → 비동기적으로 신경망 업데이트

- 여러 에이전트가 독립적으로 개별 환경에서 행동, 보상, 비평을 발생시켜 나가며 강화학습 진행
