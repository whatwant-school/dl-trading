
## History
- 1950년대 Perceptron : 인공 신경망의 시초
  - 1969년 Perceptrons (MIT) 책 발간 : Perceptron의 한계점 & 증명 -> 암흑기
- 1986년 오차 역전파법(back propagation)을 이용한 다층 인공신경망 학습 방법 고안
- 1990년대 발전된 다양한 인공신경망 등장 : 고급 순환 신경망(LSTM, 1997), 고급 합성곱 신경망(LeNet-5, 1998)
- 2000년대 딥러닝(deep learning)
- 2010년대 구글 딥마인드의 알파고


## Deep Learning으로 풀고자 하는 문제
- 분류 (classification)
- 군집화 (clustering)
- 회귀 (regression) - 불완전한 데이터의 값(value) 구하기


## 학습 방법
- 지도학습 (supervised learning) : 주로 분류/회귀 문제에 적합
- 비지도학습 (unsupervised learning) : 주로 군집화에 적합
- 강화학습 (reinforcement learning) : 주로 분류/회귀 문제에 적합


## Deep Learning 발전 과정

1. Perceptron
    - 임계치보다 크면 1, 아니면 0
  
1. Artificial Neural Network (인공 신경망)
    - 입력층/은닉층/출력층 구성, 은닉층에는 다양한 활성화 함수 존재
      - 계단 함수 : 0보다 크면 1, 작으면 -1
      - 렐루 함수 : 0보다 크면 값 그대로, 작으면 0
      - 리키렐루 함수 : 0보다 크면 값 그대로, 작으면 작은 비율로 반영(예: 0.3배)
      - 선형 함수 : 입력과 출력이 직선(그대로)
      - 시그모이드 함수 : S자 모양, 0과 1사이의 값
      - 쌍곡탄젠트 함수 : S자 모양, -1과 1사이의 값
    - 출력층에서도 함수 사용
      - 시그모이드 함수 : 주로 이항 분류 문제
      - 소프트 맥스(softmax) 함수 : 다항 분류 문제
    
1. Deep Neural Network (심층 신경망)
    - 은닉층이 2개 이상
    - 은닉층이 많아지면 컴퓨팅 비용 상승, 학습 추적 어려움


## 딥러닝에 필요한 핵심 기술

### 오차 역전파 기법
가장 마지막 계층부터 앞쪽 계층으로 순전파(forward)를 편미분한 값인 역전파(backward)를 곱해 나가면서 그 최종 역전파 값을 가중치에 더해서 조정

### 최적해 탐색 기법
- 손실(loss) = 신경망 출력과 레이블의 차이
- 손실 함수(loss function)
  - 평균제곱합 (mean squared error, MSE)
  - 교차 엔트로피 (cross entropy)


## 신경망 학습방법

### 경사 하강법 (gradient descent, GD)
- 손실 함수의 기울기에 학습 속도(learning rate)를 곱해 가중치에서 뺌으로써 가중치 갱신
- 이 과정을 반복했을 때 손실이 줄어들어야 하며, 손실에 변화가 거의 없을 때는 학습 종료
- 학습 속도를 크게 잡으면 최적화가 잘 안되고
- 학습 속도를 작게 잡으면 학습 소요 시간 증가
- 권장) 학습 초기에는 학습 속도를 크게 잡고, 후반으로 갈 수록 학습 속도를 줄이는 방법

### 확률적 경사 하강법 (stochastic gradient descent, SGD)
- 경사 하강법과 기본적인 사항은 동일
- 가중치 갱신을 할 때 샘플 하나 하나마다 하지 않고 'mini batch'라고 하는 학습 데이터 부분 집합 단위로 가중치 갱신
- 대비하여 적은 학습량이 적고, 수행 속도가 빠르지만 정확도가 조금 떨어짐


## 과적합 (overfitting)

### 정규화
- 손실 함수에 람다(lambda) 상수와 가중치 제곱의 합을 곱해 더하는 방법

### 드롭 아웃
- 신경망의 일부 뉴런을 생략하고 학습 



## 고급 인공 신경망

### 순환 신경망 (recurrent neural network, RNN)
- 은닉층에서의 출력값을 다음 학습 때 입력에 추가 = 이전의 상태를 기억해 학습에 반영
- 음성, 문장 등의 순차적 데이터 학습에 적합

### LSTM 신경망
- 순환 신경망(RNN)의 일종, 이전 상태를 더 잘 기억할 수 있도록 개선

### 합성곱 신경망 (convolutional neural network, CNN)
- 입력 계층 다음에 컨벌루션/렐루/풀링 계층이 처리 되고 '완전 연결(fully connected)' 계층이 이어지고 최종적으로 소프트맥스 계층을 거쳐서 결과 출력
- 이미지 데이터 학습에 적합


## 딥러닝 적용 사례

### 기계 번역
- 통계 기반 번역 (statistical machine translation, SMT)
- 인공 신경망 기반 번역 (neural machine translation, NMT)

### 음성 인식 (speech to text, STT)
- 주로 순환 신경망 적용

### 이미지 인식
- ImageNet : 레이블링 된 이미지 데이터베이스


## 정오표
- 37p line-5 : 교차 엔트로피(corss entropy) -> 교차 엔트로피 (cross entropy)
